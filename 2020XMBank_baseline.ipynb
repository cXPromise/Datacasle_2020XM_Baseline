{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里依次获取主办方提供的数据。为了后面做特征方便，增加了mon和season字段，并且当读取测试集时对mon和season进行了特别的处理，保证了测试集发生时间在训练集之后。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Q3_3 = pd.read_csv('y_train_3/y_Q3_3.csv')\n",
    "y_Q4_3 = pd.read_csv('y_train_3/y_Q4_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aum_m10.csv\n",
      "aum_m11.csv\n",
      "aum_m12.csv\n",
      "aum_m7.csv\n",
      "aum_m8.csv\n",
      "aum_m9.csv\n",
      "aum_m1.csv\n",
      "aum_m2.csv\n",
      "aum_m3.csv\n"
     ]
    }
   ],
   "source": [
    "aum_fils = os.listdir('x_train/aum_train/')+os.listdir('x_test/aum_test/')\n",
    "aum = []\n",
    "for f in aum_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/aum_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/aum_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    aum.append(tmp)\n",
    "aum = pd.concat(aum, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior_m10.csv\n",
      "behavior_m11.csv\n",
      "behavior_m12.csv\n",
      "behavior_m7.csv\n",
      "behavior_m8.csv\n",
      "behavior_m9.csv\n",
      "behavior_m1.csv\n",
      "behavior_m2.csv\n",
      "behavior_m3.csv\n"
     ]
    }
   ],
   "source": [
    "behavior_fils = os.listdir('x_train/behavior_train/')+os.listdir('x_test/behavior_test/')\n",
    "behavior = []\n",
    "for f in behavior_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/behavior_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/behavior_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    behavior.append(tmp)\n",
    "behavior = pd.concat(behavior, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big_event_Q3.csv\n",
      "big_event_Q4.csv\n",
      "big_event_Q1.csv\n"
     ]
    }
   ],
   "source": [
    "event_fils = os.listdir('x_train/big_event_train/')+os.listdir('x_test/big_event_test/')\n",
    "event = []\n",
    "for f in event_fils:\n",
    "    print(f)\n",
    "    season = int((f.split('.')[0]).split('_')[-1].replace('Q', ''))\n",
    "    if season>=3:\n",
    "        tmp = pd.read_csv('x_train/big_event_train/'+f)\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/big_event_test/'+f)\n",
    "    tmp['season'] = season\n",
    "    event.append(tmp)\n",
    "event = pd.concat(event, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cunkuan_m10.csv\n",
      "cunkuan_m11.csv\n",
      "cunkuan_m12.csv\n",
      "cunkuan_m7.csv\n",
      "cunkuan_m8.csv\n",
      "cunkuan_m9.csv\n",
      "cunkuan_m1.csv\n",
      "cunkuan_m2.csv\n",
      "cunkuan_m3.csv\n"
     ]
    }
   ],
   "source": [
    "cunkuan_fils = os.listdir('x_train/cunkuan_train/')+os.listdir('x_test/cunkuan_test/')\n",
    "cunkuan = []\n",
    "for f in cunkuan_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/cunkuan_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/cunkuan_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    cunkuan.append(tmp)\n",
    "cunkuan = pd.concat(cunkuan, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_avli_Q3 = pd.read_csv('x_train/cust_avli_Q3.csv')\n",
    "cust_avli_Q4 = pd.read_csv('x_train/cust_avli_Q4.csv')\n",
    "cust_info_Q3 = pd.read_csv('x_train/cust_info_Q3.csv')\n",
    "cust_info_Q4 = pd.read_csv('x_train/cust_info_Q4.csv')\n",
    "\n",
    "cust_avli_Q1 = pd.read_csv('x_test/cust_avli_Q1.csv')\n",
    "cust_info_Q1 = pd.read_csv('x_test/cust_info_Q1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76170, 2), (76722, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = y_Q4_3.copy()\n",
    "test = cust_avli_Q1.copy()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一组特征很自然的想到用户历史的label，例如在预测季度4的用户时，使用用户在季度3的label作为特征。可以简单看到这个特征的kappa值可以达到0.238+。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Q3_3 = y_Q3_3.rename(columns={'label': 'bef_label'})\n",
    "train = train.merge(y_Q3_3, on=['cust_no'], how='left')\n",
    "\n",
    "y_Q4_3 = y_Q4_3.rename(columns={'label': 'bef_label'})\n",
    "test = test.merge(y_Q4_3, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23896181609901146"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score((train['label']+1), (train['bef_label'].fillna(1)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来可以拼接下用户的基础特征，这里我只是对一些类别变量做了LabelEncoder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(cust_info_Q4, on=['cust_no'], how='left')\n",
    "test = test.merge(cust_info_Q1, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [f for f in train.select_dtypes('object').columns if f not in ['label', 'cust_no']]:\n",
    "    train[col].fillna('-1', inplace=True)\n",
    "    test[col].fillna('-1', inplace=True)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[[col]], test[[col]]], axis=0, ignore_index=True))\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76170, 23), (76722, 22))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这题最重要的应该是用户行为相关的数据，下面我们开始做一些简单的操作：\n",
    "1. 用户当季度存款（cunkuan）的mean、max、min、std、sum、last的统计\n",
    "2. 用户当季度最后一个月的aum数据\n",
    "3. 用户当季度最后一个月的behavior数据\n",
    "4. 用户当季度的event的特征，这里大多数都是时间，所以用该季度月末的后一天做时间差特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cunkuan['C3'] = cunkuan['C1'] / cunkuan['C2']\n",
    "cunkuan = cunkuan.sort_values(by=['cust_no', 'mon']).reset_index(drop=True)\n",
    "\n",
    "agg_stat = {'C1': ['mean', 'max', 'min', 'std', 'sum', 'last'],\n",
    "            'C2': ['mean', 'sum', 'min', 'max', 'std', 'last'],\n",
    "            'C3': ['mean', 'max', 'min', 'std', 'sum', 'last']}\n",
    "group_df = cunkuan[(cunkuan['mon']<=12)&(cunkuan['mon']>=10)].groupby(['cust_no']).agg(agg_stat)\n",
    "group_df.columns = [f[0]+'_'+f[1] for f in group_df.columns]\n",
    "group_df.reset_index(inplace=True)\n",
    "train = train.merge(group_df, on=['cust_no'], how='left')\n",
    "\n",
    "group_df = cunkuan[(cunkuan['mon']<=15)&(cunkuan['mon']>=13)].groupby(['cust_no']).agg(agg_stat)\n",
    "group_df.columns = [f[0]+'_'+f[1] for f in group_df.columns]\n",
    "group_df.reset_index(inplace=True)\n",
    "test = test.merge(group_df, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = [f for f in aum.columns if f.startswith('X')]\n",
    "aum['X_sum'] = aum[X_cols].sum(axis=1)\n",
    "aum['X_num'] = (aum[X_cols]>0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "tmp = aum[aum['mon']==12].copy()\n",
    "del tmp['mon']\n",
    "train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "\n",
    "tmp = aum[aum['mon']==15].copy()\n",
    "del tmp['mon']\n",
    "test = test.merge(tmp, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior['B5-B3'] = behavior['B5'] - behavior['B3']\n",
    "tmp = behavior[behavior['mon']==12].copy()\n",
    "del tmp['mon']\n",
    "train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "\n",
    "tmp = behavior[behavior['mon']==15].copy()\n",
    "del tmp['mon']\n",
    "test = test.merge(tmp, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['B6_gap'] = (pd.to_datetime('2020-01-01 00:00:00') - pd.to_datetime(train['B6'])).dt.total_seconds()\n",
    "test['B6_gap'] = (pd.to_datetime('2020-04-01 00:00:00') - pd.to_datetime(test['B6'])).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['B6_hour'] = pd.to_datetime(train['B6']).dt.hour\n",
    "test['B6_hour'] = pd.to_datetime(test['B6']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_cols = [f for f in event.columns if f.startswith('E')]\n",
    "event['event_num'] = len(E_cols) - event[E_cols].isnull().sum(axis=1)\n",
    "\n",
    "tmp = event[event['season']==4].copy()\n",
    "del tmp['season']\n",
    "train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "\n",
    "tmp = event[event['season']==1].copy()\n",
    "del tmp['season']\n",
    "test = test.merge(tmp, on=['cust_no'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in E_cols:\n",
    "    if col not in ['E15', 'E17']:\n",
    "        train[col] = (pd.to_datetime('2020-01-01 00:00:00') - pd.to_datetime(train[col])).dt.days\n",
    "        test[col] = (pd.to_datetime('2020-04-01 00:00:00') - pd.to_datetime(test[col])).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就构成了我们baseline的基础特征，下面开始训练模型。这里采用的是Lightgbm进行5折的多分类，早停直接使用kappa值。因为训练多分类时，目标值的最小值得是0，所以我们对原始label做+1的处理（记得提交的时候要改回来）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(preds, train_data):\n",
    "    y_true = train_data.label\n",
    "    preds = np.argmax(preds.reshape(3, -1), axis=0)\n",
    "    score = cohen_kappa_score(y_true, preds)\n",
    "    return 'kappa', score, True\n",
    "\n",
    "def LGB_classfication_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['cust_no', 'label', 'I7', 'I9', 'B6']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    oof_probs = np.zeros((train.shape[0], 3))\n",
    "    output_preds = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    offline_score = []\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y,\n",
    "                            )\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y)\n",
    "        parameters = {\n",
    "            'learning_rate': 0.05,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'None',\n",
    "            'num_leaves': 63,\n",
    "            'num_class': 3,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'verbose': -1,\n",
    "            'nthread': 12\n",
    "        }\n",
    "        lgb_model = lgb.train(\n",
    "            parameters,\n",
    "            dtrain,\n",
    "            num_boost_round=5000,\n",
    "            valid_sets=[dval],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100,\n",
    "            feval=kappa,\n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        oof_preds[test_index] = np.argmax(lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration), axis=1)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['kappa'])\n",
    "        output_preds.append(lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration))\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-KAPPA score:%.6f, OOF-STD:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(15))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(target, oof_preds))\n",
    "    print('classfication report:')\n",
    "    print(classification_report(target, oof_preds))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current num of features: 75\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.391214\n",
      "[200]\tvalid_0's kappa: 0.407406\n",
      "[300]\tvalid_0's kappa: 0.407399\n",
      "Early stopping, best iteration is:\n",
      "[284]\tvalid_0's kappa: 0.409634\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.392955\n",
      "[200]\tvalid_0's kappa: 0.407885\n",
      "[300]\tvalid_0's kappa: 0.410009\n",
      "[400]\tvalid_0's kappa: 0.412524\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's kappa: 0.414518\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.391538\n",
      "[200]\tvalid_0's kappa: 0.405639\n",
      "[300]\tvalid_0's kappa: 0.40816\n",
      "[400]\tvalid_0's kappa: 0.411267\n",
      "[500]\tvalid_0's kappa: 0.412\n",
      "[600]\tvalid_0's kappa: 0.412379\n",
      "Early stopping, best iteration is:\n",
      "[535]\tvalid_0's kappa: 0.414253\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.391315\n",
      "[200]\tvalid_0's kappa: 0.408037\n",
      "[300]\tvalid_0's kappa: 0.409429\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's kappa: 0.410795\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.392679\n",
      "[200]\tvalid_0's kappa: 0.407817\n",
      "[300]\tvalid_0's kappa: 0.4105\n",
      "[400]\tvalid_0's kappa: 0.411369\n",
      "[500]\tvalid_0's kappa: 0.413088\n",
      "[600]\tvalid_0's kappa: 0.41554\n",
      "[700]\tvalid_0's kappa: 0.416812\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's kappa: 0.419653\n",
      "OOF-MEAN-KAPPA score:0.413771, OOF-STD:0.003503\n",
      "feature importance:\n",
      "feature\n",
      "X_sum        81101.494288\n",
      "B6_gap       35705.408467\n",
      "bef_label    28336.175362\n",
      "C1_std       25292.655775\n",
      "C1_last      24126.071295\n",
      "C2_last      22143.340695\n",
      "C1_min       19387.615105\n",
      "B7           15646.010900\n",
      "C3_std       12826.683402\n",
      "E16          12518.379958\n",
      "B6_hour      11938.840487\n",
      "E1           11709.834091\n",
      "X3           11251.225545\n",
      "E6           10824.601992\n",
      "E18           9626.040079\n",
      "Name: importance, dtype: float64\n",
      "confusion matrix:\n",
      "[[ 6201  1203  4183]\n",
      " [ 1271  4121  9795]\n",
      " [ 1459  2423 45514]]\n",
      "classfication report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.54      0.60     11587\n",
      "           1       0.53      0.27      0.36     15187\n",
      "           2       0.77      0.92      0.84     49396\n",
      "\n",
      "    accuracy                           0.73     76170\n",
      "   macro avg       0.66      0.58      0.60     76170\n",
      "weighted avg       0.71      0.73      0.71     76170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = train['label'] + 1\n",
    "lgb_preds, lgb_oof, lgb_score = LGB_classfication_model(train, target, test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线上提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.756928\n",
       "-1    0.123120\n",
       " 0    0.119953\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = test[['cust_no']].copy()\n",
    "sub_df['label'] = np.argmax(np.mean(lgb_preds, axis=0), axis=1) - 1\n",
    "sub_df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('baseline_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_no</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0x3b9b4615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0x3b9ae61b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x3b9add69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x3b9b3601</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0x3b9b2599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_no  label\n",
       "0  0x3b9b4615      0\n",
       "1  0x3b9ae61b      1\n",
       "2  0x3b9add69      0\n",
       "3  0x3b9b3601      1\n",
       "4  0x3b9b2599      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
